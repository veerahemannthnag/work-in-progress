request.json

{
  "projectId": "veerahemannthnag",
  "job": {
    "placement": {
      "clusterName": "cluster"
    },
    "statusHistory": [],
    "reference": {
      "jobId": "job-0efb8d1e",
      "projectId": "veerahemannthnag"
    },
    "pysparkJob": {
      "mainPythonFileUri": "gs://veerahemannthnag-9988999994394/helloworld.py",
      "properties": {}
    }
  }
}

create request.json file with above content

helloworld.py
#!/usr/bin/python
import pyspark
sc = pyspark.SparkContext()
rdd = sc.parallelize(['Hello,', 'world!'])
words = sorted(rdd.collect())
print(words)


curl -X POST -H "Authorization: Bearer "$(gcloud auth application-default print-access-token) -H "Content-Type: application/json; charset=utf-8" -d @request.json "https://dataproc.googleapis.com/v1/projects/veerahemannthnag/regions/us-central1/jobs:submit"

or

curl -X POST \
-H "Authorization: Bearer "$(gcloud auth application-default print-access-token) \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://dataproc.googleapis.com/v1/projects/veerahemannthnag/regions/us-central1/jobs:submit"


Reference:
https://cloud.google.com/dataproc/docs/guides/submit-job#dataproc-submit-job-drest
